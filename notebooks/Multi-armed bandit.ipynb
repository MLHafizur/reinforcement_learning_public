{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Multiarmed bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "\"[Reinforcement learning (RL)](https://en.wikipedia.org/wiki/Reinforcement_learning)\" is a very interesting sub-field of Machine Learning. There have been many new developments in RL in the last 5 years, starting from the 2013 publication of \"[Deep Q-Networks](https://deepmind.com/research/dqn/)\" from DeepMind. In this post we will look at an important concept in RL called the tradeoff between exploration and exploitation. We will simulate a problem called \"[Multi-armed bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit#The_multi-armed_bandit_model)\" and understand the details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals of the post\n",
    "* Share some good resources on RL.\n",
    "* Simulate K-Armed bandit problem\n",
    "* Understand the tradeoff between Exploration and Exploitation in RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several great resources that became available only recently to learn about RL. Below are some of the best ones I found for practitioners like myself. These are good starting points for understanding the foundations and learning by doing. Richard Sutton and Andrew Barto's \"[second edition](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249/ref=sr_1_1_sspa?ie=UTF8&qid=1544374069&sr=8-1-spons&keywords=reinforcement+learning&psc=1)\" is beautifully written. I really like how they provide intuitive explanation of algorithms and the pseudo-code. The pseudo-code is proving to be invaluable when I want to code up an algorithm and understand the details. Andrej Karpathy wrote an \"[excellent post](http://karpathy.github.io/2016/05/31/rl/)\" almost two years ago. It's a great general introduction and also a good starting point for a type of RL aglorithms called Policy gradient methods. Finally I found \"[this](http://rail.eecs.berkeley.edu/deeprlcourse/)\" course from Berkeley that is very recent with full lecture notes and videos available. I have only reviewed one lecture so far, but it looks very promising. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before we begin ... [TBD]**\n",
    "**Note**: All of the code for this post can be found here[TBD]\n",
    "**Requirements**: Docker and docker-compose. [TBD] \n",
    "**Method to run**: [TBD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-armed bandit problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general setting of RL is shown below. At time t, the Agent observes state $S_t$ from the environment and also receives a reward $R_t$. The agent then takes an action $A_t$. In response to action $A_t$, the environment provides the next state and reward pair and the process continues. This setup represents what is called a Markov Decision Process. The goal of the agent is to maximize the cumulative reward it receives from the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RL_MDP](images/rl_mdp.png)\n",
    "\n",
    "<small>(Source: [Richard Sutton](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249/ref=sr_1_1_sspa?ie=UTF8&qid=1544374069&sr=8-1-spons&keywords=reinforcement+learning&psc=1).)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most distinguishing feature of RL compared to supervised learning is that there are no labels associated with actions; there is only reward for each action taken. Supervised learning can be thought of as 'Instructive Learning' whereas RL can be thought of as 'Evaluative Learning'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we are considering a 10-armed bandit problem. This means that at every time step, the agent can choose from one of 10 possible actions. After choosing an action, the agent receives a reward that is drawn from an unknown (to the agent) probability distribution corresponding to the said action. The goal of the agent is to maximize the total received rewards within a certain number of timesteps, say 3000. Note that the environment can be in only one state. This simplifies the the problem considerably and makes the successive steps IID.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to see this through an example. We are using this library that simulates the 10 armed bandit problem described in the book. \n",
    "\n",
    "https://github.com/JKCooper2/gym-bandits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to OpenAI gym can be found here. \n",
    "https://www.oreilly.com/learning/introduction-to-reinforcement-learning-and-openai-gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The set of distributions our environment is using to reward our agent could like this for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![10-armed-ex-dists](images/10-armed-ex-dists.png)\n",
    "\n",
    "<small>(Source: [Richard Sutton](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249/ref=sr_1_1_sspa?ie=UTF8&qid=1544374069&sr=8-1-spons&keywords=reinforcement+learning&psc=1).)</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are two popular approaches to RL today\n",
    "    * Action-value methods\n",
    "    * Policy gradient methods\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are focusing on Action-value methods in this post. Within Action-value methods can be further divided into two types:\n",
    "* Tabular methods\n",
    "* Function-approximation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-armed bandit problem can be solved using Tabular method. It is quite simple: We maintain a table of actions and their estimated average reward. The agent then uses this table to take actions based on some policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest policies is *the geedy* policy where the agent always chooses the action with the maximum expected return. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RL_MDP](images/action_values_eq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RL_MDP](images/greedy_policy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is called epsilon-greedy policy which takes action using the greedy policy with a probability of 1-epsilon and a random action with a probability of epsilon. This approach ensures all the action space is \"explored\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define yet another policy called decaying-epsilon-greedy method, where we slowly decay the epsilon overtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"400\" src=\"../reports/figures/10_armed_agents_actions.html\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=\"400\" src=\"../reports/figures/10_armed_agents_actions.html\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"400\" src=\"../reports/figures/10_armed_agents_rewards.html\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=\"400\" src=\"../reports/figures/10_armed_agents_rewards.html\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
